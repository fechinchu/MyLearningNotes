# 4.4.4.Zookeeper

# 1.Zookeeper简介

Apache Zookeeper是一种用于分布式应用程序的**高性能协调服务**,提供一种集中式信息存储服务;

特点:数据存在内存中,类型文件系统的树形结构(文件和目录),高吞吐和低延迟,集群高可靠;

作用:基于Zookeeper可以实现分布式统一配置中心,服务注册中心,分布式锁等; 

<img src="https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210401150611173.png" alt="image-20210401150611173" style="zoom:50%;" />

Zookeeper的应用案例

* Hbase:使用Zookeeper进行Master选举,服务间协调;
* Solr:使用Zookeeper进行集群管理,Leader选举,配置管理;
* Dubbo:使用Zookeeper服务注册与发现;
* MyCat:集群管理,配置管理;

# 2.Zookeeper核心概念

## 2.1.session

* 一个客户端连接一个会话,由zk分配唯一sessionId;
* 客户端以特定的时间间隔发送心跳以保持会话有效;
* 超过会话超时时间未收到客户端的心跳,则判定客户端死了;
* 会话中的请求按FIFO顺序执行;

![image-20210402142022504](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210402142022504.png)

## 2.2.数据模型

* 层次名称空间
  * 类似unit文件系统,以'/'为根;
  * 节点可以包含与之关联的数据以及子节点(既是文件也是文件夹);
  * 节点的路径总是表示为规范的,绝对的,斜杠分隔的路径;
* znode
  * 名称唯一,命名规范;
  * 节点类型:持久,顺序,临时,临时顺序;
  * 节点数据构成;

![image-20210402142452026](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210402142452026.png)

### 2.2.1.znode节点类型

* 持久节点`create /app1 xxx`;
* 临时节点 `create -e /app2 xxx`,当会话关闭,临时节点就会被删除;
* 顺序节点`create -s /app1/a xxx`,`create -s /app1/ xxx`;分别得到两个节点`a0000000000`,`00000000001`;
* 临时顺序节点:`create -s -e /app1/xxx`

### 2.2.2.znode数据构成;

* 节点元数据:stat结构;
* 节点数据:储存的协调数据;

![image-20210402145728017](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210402145728017.png)

<img src="https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210402150301081.png" alt="image-20210402150301081" style="zoom: 33%;" />

注意:其中的aclVersion是访问控制列表的版本;

Zookeeper uses ACLs to control access to its znode;

![image-20210402151752905](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210402151752905.png)

## 2.3.Zookeeper中的时间

Zookeeper中有多种方式跟踪时间;

* Zxid:Zookeeper中的每次更改操作都对应一个唯一的事务id,称为Zxid,它是一个全局有序的戳记,如果zxid1小于zxid2,那么zxid1发生在zxid2之前;
* Version numbers:版本号,对节点的每次更改都会导致该节点的版本号之一增加;
* Ticks:当使用多服务器Zookeeper时,服务器使用'滴答'来定义事件的时间,如状态上传,会话超时,对等点之间的连接超时等;
* Real time:Zookeeper除了在znode创建和修改时将时间戳放入stat结构之外,根本不使用Real time或时钟时间;

## 2.4.Watch监听机制

客户端可以在znodes上设置watch,监听znode的变化;

当我们使用`-w`来开启app1 的watch监听机制的时候,我们修改app1的数据.就可以监听到watch;

![image-20210402154820441](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210402154820441.png)

两类watch:

* data watch监听数据变更;
* child watch监听子节点变化;

触发watch事件(getData,getChildren,exists)

* created event:
  * enabled with a call to exists;
* deleted event:
  * enabled with a call to exists,getData,and getChildren;
* changed event:
  * enabled with a call to exists and getData;
* child event:
  * enabled with a call to getChildren;

watch重要特性:

* 一次性触发:watch触发后即被删除,要持续监控变化,则需要持续设置watch;
* 有序性:客户端先得到watch通知,后才会看到变化结果;

watch注意事项:

* watch是一次性触发器;如果您获得了一个watch事件,并且希望得到关于未来更改的通知,则必须设置另一个watch;
* 因为watch是一次性触发器,并且在获取事件和发送获取watch的新请求之间存在延时,所以不能可靠地得到节点发生的每个更改;
* 一个watch对象只会被特定的通知触发一次.如果一个watch对象同时注册了exists,getData,当节点被删除时,删除事件对exists,getData都有效,但只会调用watch一次;

## 2.5.Zookeeper特性

1. 顺序一致性(Sequential Consistency):保证客户端操作是按顺序生效的;
2. 原子性(Atomicity):更新成功或失败,没有部分结果;
3. 单个系统印象:无论连接到哪个服务器,客户端都将看到相同的内容;
4. 可靠性:数据的变更不会丢失,除非被客户端覆盖修改;
5. 及时性:保证系统的客户端当时读取到的数据是最新的;

# 3.Zookeeper应用场景

## 3.1.Zookeeper典型应用场景

* 数据发布订阅(配置中心);
* 命名服务;
* Master选举;
* 集群管理;
* 分布式队列;
* 分布式锁;

## 3.2.Zookeeper实现配置中心

![image-20210720234310847](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210720234310847.png)

  通过zookeeper的znode能够存储数据和watcher监听机制来实现分布式配置中心;

## 3.3.Zookeeper实现命名服务

<img src="https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210405182837501.png" alt="image-20210405182837501" style="zoom:50%;" />

## 3.4.Zookeeper实现Master选举

![image-20210405183324616](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210405183324616.png)

~~~java
public class MasterElectionDemo {

	static class Server {

		private String cluster, name, address;

		private final String path, value;

		private String master;

		public Server(String cluster, String name, String address) {
			super();
			this.cluster = cluster;
			this.name = name;
			this.address = address;
			path = "/" + this.cluster + "Master";
			value = "name:" + name + " address:" + address;

			ZkClient client = new ZkClient("localhost:2181");
			client.setZkSerializer(new MyZkSerializer());

			//该线程,要改为守护线程
			new Thread(new Runnable() {

				@Override
				public void run() {
					electionMaster(client);
				}

			}).start();
		}

		public void electionMaster(ZkClient client) {
			try {
				client.createEphemeral(path, value);
				master = client.readData(path);
				System.out.println(value + "创建节点成功，成为Master");
			} catch (ZkNodeExistsException e) {
				master = client.readData(path);
				System.out.println("Master为：" + master);
			}

			// 为阻塞自己等待而用
			CountDownLatch cdl = new CountDownLatch(1);

			// 注册watcher
			IZkDataListener listener = new IZkDataListener() {

				@Override
				public void handleDataDeleted(String dataPath) throws Exception {
					System.out.println("-----监听到节点被删除");
					cdl.countDown();
				}

				@Override
				public void handleDataChange(String dataPath, Object data) throws Exception {

				}
			};

			client.subscribeDataChanges(path, listener);

			// 让自己阻塞
			if (client.exists(path)) {
				try {
					cdl.await();
				} catch (InterruptedException e1) {
					e1.printStackTrace();
				}
			}
			// 醒来后，取消watcher
			client.unsubscribeDataChanges(path, listener);
			// 递归调自己（下一次选举）
			electionMaster(client);

		}
	}
}
~~~



## 3.5.Zookeeper实现分布式队列

![image-20210405185108317](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210405185108317.png)

## 3.6.Zookeeper实现分布式锁

### 3.6.1.分布式锁的实现方式一

<img src="https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210405192341844.png" alt="image-20210405192341844" style="zoom:33%;" />

<img src="https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210405192441451.png" alt="image-20210405192441451" style="zoom:33%;" />

缺点:惊群效应;

疑惑:为什么不通过zookeeper的watch机制进行阻塞等待,后来发现注册节点的watcher并不会阻塞;

~~~java
public class ZKDistributeLock implements Lock {

	private String lockPath;

	private ZkClient client;

	// 锁重入计数
	private ThreadLocal<Integer> reentrantCount = new ThreadLocal<>();

	public ZKDistributeLock(String lockPath) {
		super();
		this.lockPath = lockPath;

		client = new ZkClient("47.117.141.206:2181");
		client.setZkSerializer(new MyZkSerializer());
	}

	@Override
	public boolean tryLock() { // 不会阻塞

		if (this.reentrantCount.get() != null) {
			int count = this.reentrantCount.get();
			if (count > 0) {
				this.reentrantCount.set(++count);
				return true;
			}
		}
		// 创建节点
		try {
			client.createEphemeral(lockPath);
			this.reentrantCount.set(1);
		} catch (ZkNodeExistsException e) {
			return false;
		}
		return true;
	}

	@Override
	public void unlock() {
		// 重入的释放锁处理
		if (this.reentrantCount.get() != null) {
			int count = this.reentrantCount.get();
			if (count > 1) {
				this.reentrantCount.set(--count);
				return;
			} else {
				this.reentrantCount.set(null);
			}
		}
		client.delete(lockPath);
	}

	@Override
	public void lock() { // 如果获取不到锁，阻塞等待
		if (!tryLock()) {
			// 没获得锁，阻塞自己
			waitForLock();
			// 再次尝试
			lock();
		}

	}

	private void waitForLock() {

		CountDownLatch cdl = new CountDownLatch(1);

		IZkDataListener listener = new IZkDataListener() {

			@Override
			public void handleDataDeleted(String dataPath) throws Exception {
				System.out.println("----收到节点被删除了-------------");
				cdl.countDown();
			}

			@Override
			public void handleDataChange(String dataPath, Object data) throws Exception {
			}
		};

		client.subscribeDataChanges(lockPath, listener);

		// 阻塞自己
		if (this.client.exists(lockPath)) {
			try {
				cdl.await();
			} catch (InterruptedException e) {
				e.printStackTrace();
			}
		}
		// 取消注册
		client.unsubscribeDataChanges(lockPath, listener);
	}
}
~~~

### 3.6.2.分布式锁的实现方式二

<img src="https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210406111116854.png" alt="image-20210406111116854" style="zoom:33%;" />

<img src="https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210406111245906.png" style="zoom:33%;" /> 

~~~java
public class ZKDistributeImproveLock implements Lock {

	/*
	 * 利用临时顺序节点来实现分布式锁
	 * 获取锁：取排队号（创建自己的临时顺序节点），然后判断自己是否是最小号，如是，则获得锁；不是，则注册前一节点的watcher,阻塞等待
	 * 释放锁：删除自己创建的临时顺序节点
	 */
	private String lockPath;

	private ZkClient client;

	private ThreadLocal<String> currentPath = new ThreadLocal<>();

	private ThreadLocal<String> beforePath = new ThreadLocal<>();

	// 锁重入计数
	private ThreadLocal<Integer> reentrantCount = new ThreadLocal<>();

	public ZKDistributeImproveLock(String lockPath) {
		super();
		this.lockPath = lockPath;
		client = new ZkClient("47.117.141.206:2181");
		client.setZkSerializer(new MyZkSerializer());
		if (!this.client.exists(lockPath)) {
			try {
				this.client.createPersistent(lockPath);
			} catch (ZkNodeExistsException e) {

			}
		}
	}

	@Override
	public boolean tryLock() {
		if (this.reentrantCount.get() != null) {
			int count = this.reentrantCount.get();
			if (count > 0) {
				this.reentrantCount.set(++count);
				return true;
			}
		}

		if (this.currentPath.get() == null) {
			currentPath.set(this.client.createEphemeralSequential(lockPath + "/", "aaa"));
		}
		// 获得所有的子
		List<String> children = this.client.getChildren(lockPath);

		// 排序list
		Collections.sort(children);

		// 判断当前节点是否是最小的
		if (currentPath.get().equals(lockPath + "/" + children.get(0))) {
			this.reentrantCount.set(1);
			return true;
		} else {
			// 取到前一个
			// 得到字节的索引号
			int curIndex = children.indexOf(currentPath.get().substring(lockPath.length() + 1));
			beforePath.set(lockPath + "/" + children.get(curIndex - 1));
		}
		return false;
	}

	@Override
	public void lock() {
		if (!tryLock()) {
			// 阻塞等待
			waitForLock();
			// 再次尝试加锁
			lock();
		}
	}

	private void waitForLock() {

		CountDownLatch cdl = new CountDownLatch(1);

		// 注册watcher
		IZkDataListener listener = new IZkDataListener() {

			@Override
			public void handleDataDeleted(String dataPath) throws Exception {
				System.out.println("-----监听到节点被删除");
				cdl.countDown();
			}

			@Override
			public void handleDataChange(String dataPath, Object data) throws Exception {

			}
		};

		client.subscribeDataChanges(this.beforePath.get(), listener);

		// 怎么让自己阻塞
		if (this.client.exists(this.beforePath.get())) {
			try {
				cdl.await();
			} catch (InterruptedException e) {
				e.printStackTrace();
			}
		}
		// 醒来后，取消watcher
		client.unsubscribeDataChanges(this.beforePath.get(), listener);
	}

	@Override
	public void unlock() {
		// 重入的释放锁处理
		if (this.reentrantCount.get() != null) {
			int count = this.reentrantCount.get();
			if (count > 1) {
				this.reentrantCount.set(--count);
				return;
			} else {
				this.reentrantCount.set(null);
			}
		}
		// 删除节点
		this.client.delete(this.currentPath.get());
	}
}
~~~

### 3.6.3.Redis实现分布式锁

* 普通的redis分布式锁
  * 加锁:`SET my:lock <随机值> NX PX 3000`,
  * 释放锁就是删除key,但是一般可以用Lua脚本删除,判断value一样才能删除

  ~~~lua
  if redis.call("get",KEYS[1]) == ARGV[1] then
  return redis.call("del",KEYS[1])
  else
      return 0
  end
  ~~~

  * 为什么要用随机值?因为如果某个客户端获取到了锁,但是阻塞了很长时间才执行完,此时可能已经自动释放锁了,此时可能别的客户端已经获取到了这个锁,要是这个时候直接删除key的话会有问题,所以得用随机值加上Lua脚本来释放锁;

  ![image-20210721210120104](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210721210120104.png)

  <img src="https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210721211952203.png" alt="image-20210721211952203" style="zoom:50%;" />

  上图实现的是可重入锁

* RedLock算法

  * 场景有一个redis cluster,有5个redis master实例,然后执行如下步骤获取一把锁;
  * 获取当前时间戳,单位是毫秒;
  * 轮流尝试在每个master节点上创建锁,过期时间较短,一般就几十毫秒;
  * 尝试在大多数节点上建立一个锁,比如5个节点就要求3个节点(n/2+1);
  * 客户端计算建立好锁的时间,如果建立锁的时间小于超时时间,就算建立成功了;
  * 要是锁建立失败了,那么就依次删除这个锁;
  * 只要别人建立了一把分布式锁,就得不断轮询去尝试获取锁;

### 3.6.4.常用的分布式锁实现技术

|               | 实现思路                                      | 优点                                     | 缺点                                                         |
| ------------- | --------------------------------------------- | ---------------------------------------- | ------------------------------------------------------------ |
| 数据库实现    | 利用数据库自身提供的锁机制,要求数据库支持行锁 | 实现简单,稳定可靠                        | 性能差,无法适应高并发场景;容易出现死锁情况;无法优雅的实现阻塞式锁; |
| 缓存实现      | 使用setnx和lua脚本机制,保证对缓存操作的原子性 | 性能非常好                               | 实现相对复杂,有出现死锁的可能性,无法优雅的实现阻塞式锁;      |
| zookeeper实现 | 基于zk的节点特性以及watch机制实现             | 性能好,稳定可靠性高,能较好的实现阻塞式锁 | 实现相对复杂;                                                |

## 3.7.Zookeeper分布式协调场景

这个其实是zk很经典的一个用法，简单来说，就好比，你A系统发送个请求到mq，然后B消息消费之后处理了。那A系统如何知道B系统的处理结果？用zk就可以实现分布式系统之间的协调工作。A系统发送请求之后可以在zk上对某个节点的值注册个监听器，一旦B系统处理完了就修改zk那个节点的值，A立马就可以收到通知，完美解决。

![image-20210720233037718](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210720233037718.png) 

## 3.8.Zookeeper HA高可用场景

比如hadoop、hdfs、yarn等很多大数据系统，都选择基于zk来开发HA高可用机制，就是一个重要进程一般会做主备两个，主进程挂了立马通过zk感知到切换到备用进程.

![image-20210720233516312](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210720233516312.png)

# 4.Zookeeper集群

## 4.1.Zookeeper集群搭建参数

1. `initLimit`

   集群中的follow服务器与leader服务器之间完成初始化同步连接时能容忍的最多心跳数.如果zk集群环境数量确实很大,同步数据的时间就会变长,因此这种情况可以适当调大该参数;

2. `syncLimit`

   集群中的follow服务器与leader服务器与leader服务器之间请求和应答之间能容忍的最多tickTime;

3. 集群节点

   `server.[id] = [host]:[port]:[port]`

   id:通过在各自的dataDir目录下创建一个名为myid的文件来为每台机器赋予一个服务器id;

   两个端口号:第一个跟随者用来连接到领导者,第二个用来选举领导者;

   注意:myid文件中一行只包含机器id的文本,id在集成中必须是唯一的,其值应该在1到255之间.如服务器1的id为"1";

## 4.2.Zookeeper集群-ZAB协议

<img src="https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210406140444264.png" alt="image-20210406140444264" style="zoom:50%;" />

ZAB协议(Zookeeper Atomic Broadcast),Zookeeper原子消息广播协议是专为zookeeper设计的数据一致性协议;

1. 所有事务请求转发给leader;
2. Leader分配全局单调递增事务id(Zxid),广播事务提议;
3. Follower处理提议,做出反馈;
4. Leader收到过半数反馈,广播commit;
5. Leader做出响应;

<img src="https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210406141207932.png" alt="image-20210406141207932" style="zoom: 33%;" />

## 4.3.Zookeeper集群-ZAB协议-崩溃恢复

Leader服务器出现崩溃,或者说由于网络原因导致Leader服务器失去了与过半Follower的联系,那么就会进入崩溃恢复模式;

* ZAB协议规定如果一个事务Proposal在一台机器上被处理成功,那么应该在所有的机器上都被处理成功,哪怕机器出现故障崩溃;
* ZAB协议确保那些已经在Leader服务器上提交的事务最终被所有服务器提交;
* ZAB协议确保丢弃那些只在Leader服务器上被提出的事务;

ZAB协议需要设计的选举算法应该满足:确保提交已经被Leader提交的事务Proposal,同时丢弃已经被跳过的事务Proposal;

* 如果让Leader选举算法能够保证新选举出来的Leader服务器拥有集群中所有机器最高ZXID的事务Proposal,那么久就可以保证这个新选举出来的Leader一定具有所有已提交的Proposal;
* 如果让具有最高编号事务Proposal机器来称为Leader,就可以省去Leader服务器检查Proposal的提交和丢弃工作的这一步操作;

## 4.4.Zookeeper集群-ZAB协议-数据同步

Leader选举出来后,需完成Followers与Leader的数据同步,当半数的Followers完成同步,则可以开始提供服务.同步过程如下:

* Leader服务器会为每个Follower服务器准备一个队列,并将那些没有被各Follower服务器同步的事务以Proposal消息的形式发送给Follower服务器,并在每一个Proposal消息后面紧接着再发送一个Commit消息,以表示该事务已经被提交;
* Follower服务器将所有其尚未同步的事务Proposal都从Leader服务器上同步过来并成功应用到本地数据库中后,Leader服务器就会将该Follower服务器加入到真正可用Follower列表中,并开始之后的其他流程;

## 4.5.Zookeeper集群-ZAB协议-丢弃事务Proposal处理

在ZAB协议的事务编号ZXID设计中,ZXID是一个64位数字.

* 低32位是一个简单的单调递增的计数器,针对客户端的每个事务请求,Leader服务器在产生一个新的事务Proposal的时候,都会对该计数器进行加1操作;
* 高32位代表了Leader周期纪元的编号,每当选举产生一个新的Leader服务器,就会从这个Leader服务器上取出其本地日志最大事务Proposal的ZXID,并从ZXID中解析出对应的纪元值,然后对其进行加1操作,之后就会以此编号作为新的纪元,并将低32位置0来开始生成新的ZXID;

基于这样的策略,当一个包含了上一个Leader周期中尚未提交过的事务Proposal的服务器启动加入到集群中,发现此集群中已经存在Leader,将自身以Follower角色连接上Leader服务器之后,Leader服务器会根据自己服务器上最后被提交的Proposal来和Follower服务器的proposal进行对比,发现follower上有上一个leader周期的事务Proposal时候,Leader会要求Follower进行一个回退操作(回退到一个确实已经被集群中过半机器提交的最新的事务Proposal);

## 4.6.Zookeeper集群-Leader选举

对选举算法的要求:

* 选出Leader节点上要持有最高的zxid;
* 过半数节点同意;

内置实现的选举算法:

* LeaderElection;
* FastLeaderElection(默认);
* AuthFastLeaderElection;

选举机制中的概念:

* 服务器id myid;
* 事务id,服务器中存放的最大Zxid;
* 逻辑时钟,发起的投票轮数计数;
* 选举状态:
  * Looking:竞选状态;
  * Following:随从状态,同步Leader状态,参与投票;
  * Observing:观察状态,同步Leader状态,不参与投票;
  * Leading,领导者状态;

选举算法:

1. 每个服务实例均发起选举自己为领导者的投票(自己的投给自己);
2. 其他服务实例收到投票邀请时,比较发起者的数据事务ID是否比自己最新的事务ID大,大则给它投票,小则不投票给它,相等则比较发起者的服务器ID,大则投票给它;
3. 发起者收到大家的投票反馈后,看投票数(含自己的)是否大于集群的半数,大于则胜出,担任领导者,未超过半数且领导者未选出,则再次发起投票;

# 5.分布式一致性算法

## 5.1.2PC

2PC:将提交的更新操作分为提交请求(投票),提交(完成)两个阶段来达成一致性算法.主要用于事务管理,分布式一致性;

<img src="https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210406153947671.png" alt="image-20210406153947671" style="zoom:33%;" />

1. 协调者询问各参与者是否可以提交该更新;等待所有参与者给出响应;
2. 参与者执行事务操作到等待提交指令的点(这个过程中记录redo,undo日志);
3. 参与者响应是否准备好提交的结果给协同者,并阻塞等待协调者的下一步指令;
4. 协调者接收所有参与者的响应,如果超时未收到响应,当成abort处理,有一个abort,下一步是回滚;
5. 协调者向所有参与者发出提交或回滚消息;
6. 参与者执行提交/回滚,释放占用的锁等资源,并作出响应;
7. 结束;

<img src="https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210406154928696.png" alt="image-20210406154928696" style="zoom: 33%;" />

2PC存在的不足:

* 上述第3步,阻塞等待协调者的下一步指令,准备完成时,如果协调者宕机,所有参与者将一直阻塞;
* 上述第5步,协调者向所有参与者发出提交或回滚消息,如果参与者宕机,将接受不到提交消息,会出现不一致,需要人工干预;

## 5.2.3PC

![image-20210406160103725](https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210406160103725.png)

 在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。 ）;

相对于2PC，3PC主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。

## 5.3.Paxos

<img src="https://fechin-picgo.oss-cn-shanghai.aliyuncs.com/PicGo/image-20210406164126293.png" alt="image-20210406164126293" style="zoom:33%;" />

* Proposer:提议者,负责提议,提出想要达成一致的value的提案;
* Acceptor:接收者,对提案投票,决定是否接受此value提案;
* Learner:学习者,不参与投票,学习投票决定的提案;

一个节点既可以是提议者,也可以是接受者;

### 5.3.1.Paxos流程

#### 5.3.1.1.Paxos写流程

1. 提案:
   1. 提案编号:唯一,保证全局递增,体现提案的先后顺序;
   2. 更新值
2. 写流程,两阶段:
   1. 准备阶段(投票阶段):
      1. 提议者提出提案给接收者;
      2. 提议者如果接收该提案,做出响应,并不再Accept比当前提案号低的提案;
      3. 提议者收到超过半数的响应,则进入下一阶段,否则丢弃提案;
   2. 接受变更阶段(提交阶段);
      1. 提议者向接收者发送Accept消息;
      2. 接收者比较Accpet消息中的提案号,如果比自己当前已接收的提案号低,则不Accept,否则Accept;

 #### 5.3.1.2.Paxos读流程

1. 接收客户端请求的节点,向集群广播获取大家的当前值;
2. 接收到过半数相同的值,则返回该值,如果本地的值不同,则更新为多数值;
3. 如果得不到过半数的相同的值,则读取失败;

### 5.3.2.基于Paxos算法的协议

1. ZAB:参考上文
2. raft:https://raft.github.io/

